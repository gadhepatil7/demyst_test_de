{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PySpark Data Generation and Anonymization\n",
    "\n",
    "This notebook demonstrates how to generate a large CSV file with random data using PySpark and then anonymize the data using User Defined Functions (UDFs).\n",
    "\n",
    "## Steps Covered:\n",
    "1. Initialize a Spark session.\n",
    "2. Generate random data including names, addresses, and dates of birth.\n",
    "3. Anonymize the generated data.\n",
    "4. Save the anonymized data to a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Initialize Spark Session and Define Helper Functions\n",
    "\n",
    "We start by initializing a Spark session and defining helper functions to generate random strings and dates.\n",
    "\n",
    "- `generate_random_string(length)`: Generates a random string of a specified length.\n",
    "- `generate_random_date(start_year, end_year)`: Generates a random date within the specified range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "235eb552-4c8f-43c3-bf18-59f9075bf22c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------------------+-------------+\n",
      "| id|first_name| last_name|             address|date_of_birth|\n",
      "+---+----------+----------+--------------------+-------------+\n",
      "|  0|dBYjDGoILL|dBYjDGoILL|ZqlMtQOHpFYYWAettINf|   1993-02-22|\n",
      "|  1|Weyeqbrpja|Weyeqbrpja|UhOdwBAqAIvkZKPKoafd|   1973-05-14|\n",
      "|  2|vTsnFQrmQt|vTsnFQrmQt|lgLUnlqlIaDyXEOSntQf|   1961-07-03|\n",
      "|  3|gPHMQOdZMC|gPHMQOdZMC|MASxhpInmJkaOBCCPdXA|   1966-01-22|\n",
      "|  4|PXLDHXcQGW|PXLDHXcQGW|auCwYcfpfYBTLGKcjysI|   1990-07-28|\n",
      "|  5|CqhwKsnlbV|CqhwKsnlbV|oahoYjbVkafrhLuwoAHe|   1972-09-13|\n",
      "|  6|IxPQnXzlon|IxPQnXzlon|bcFYrmetqptkfdEqJpmf|   1968-07-16|\n",
      "|  7|jhIzSNZcFG|jhIzSNZcFG|kSiYABGkOYvWPbAqzTsB|   1986-04-10|\n",
      "|  8|rOTyzEHqsQ|rOTyzEHqsQ|NgmxiSSqnlCOavBmebrv|   1973-01-30|\n",
      "|  9|JlzGShLXCV|JlzGShLXCV|YkdupFBHbdagGRCOIKwL|   1976-11-02|\n",
      "+---+----------+----------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, udf\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder     .appName(\"Generate Large CSV\")     .getOrCreate()\n",
    "\n",
    "# Helper function to generate random strings\n",
    "def generate_random_string(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters, k=length))\n",
    "\n",
    "# Helper function to generate random dates\n",
    "def generate_random_date(start_year=1950, end_year=2000):\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    random_date = start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "    return random_date.strftime('%Y-%m-%d')\n",
    "\n",
    "# Create a DataFrame with random data\n",
    "data = [(i, generate_random_string(10), generate_random_string(10), generate_random_string(20), generate_random_date()) for i in range(1000000)]\n",
    "columns = [\"id\", \"first_name\", \"last_name\", \"address\", \"date_of_birth\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show a sample of the data\n",
    "df.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Define UDFs for Anonymization and Apply Them\n",
    "\n",
    "In this step, we define User Defined Functions (UDFs) for anonymizing string data. We then apply these UDFs to the DataFrame columns that need to be anonymized.\n",
    "\n",
    "- `anonymize_string(length)`: Generates a random string of the specified length.\n",
    "- `anonymize_string_udf`: A UDF based on `anonymize_string` to be applied to DataFrame columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5baa7833-5ebc-48fc-aa17-f098a32ea098",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------------------+-------------+\n",
      "| id|first_name| last_name|             address|date_of_birth|\n",
      "+---+----------+----------+--------------------+-------------+\n",
      "|  0|mnlfJgnpQR|mnlfJgnpQR|nNqvuqeJQgsJdHNIlJkR|   1998-07-27|\n",
      "|  1|UFuRByqRCf|UFuRByqRCf|xSCbOtEUgkmWpCgWKdGC|   2000-04-06|\n",
      "|  2|FABIXDNUsm|FABIXDNUsm|KTtgHBsHzALAeSwNAFbB|   1998-01-15|\n",
      "|  3|pRZnuugaVb|pRZnuugaVb|fxEVMuhnypbxnkooioPW|   1994-10-29|\n",
      "|  4|myhXQuSVmO|myhXQuSVmO|DtkGclMLPhrEDNhXtfXa|   1999-05-27|\n",
      "|  5|bEQPMkGewD|bEQPMkGewD|cjZwQjXYIVGzuUviScwb|   1971-11-15|\n",
      "|  6|VnZXaKzwzt|VnZXaKzwzt|GkRxWEqaWfLJLjRmGCjd|   1982-05-30|\n",
      "|  7|qduHvUrUsY|qduHvUrUsY|XcIyvOQkARkeoLAwewDe|   1994-05-15|\n",
      "|  8|BXtpkPDOzC|BXtpkPDOzC|ykOSWobzsZNSeovweylc|   1970-08-12|\n",
      "|  9|HVCYCobMBt|HVCYCobMBt|kLiCWfMJWwPQvvSTGdnz|   1993-05-24|\n",
      "+---+----------+----------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define UDFs for anonymization\n",
    "def anonymize_string(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters, k=length))\n",
    "\n",
    "anonymize_string_udf = udf(anonymize_string, StringType())\n",
    "\n",
    "# Apply anonymization to specific columns\n",
    "anonymized_df = df.withColumn(\"first_name\", anonymize_string_udf(lit(10)))                   .withColumn(\"last_name\", anonymize_string_udf(lit(10)))                   .withColumn(\"address\", anonymize_string_udf(lit(20)))\n",
    "\n",
    "# Save the anonymized DataFrame to a new CSV file\n",
    "anonymized_df.write.csv(\"anonymized_large_dataset_spark_2.csv\", header=True)\n",
    "\n",
    "# Display the first 10 rows of the anonymized DataFrame\n",
    "anonymized_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b641ebd-383e-4dc4-9840-8629fb8eeb89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[object Object]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-08-12 12:27:51",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
